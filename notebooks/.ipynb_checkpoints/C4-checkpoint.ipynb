{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Million Song Dataset Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline tests concluded that the system with batch normalization achieved the best results. --CONTINUE--\n",
    "\n",
    "Test to include in making the system better:\n",
    "\n",
    "* Feedforward network with different input data\n",
    "    * Lower number of segments (Reduce from 120 to 100, 80, 60, 40, 20) with **best baseline configuration**.\n",
    "    * Lower number of segments with **computationally efficient best baseline configuration**.\n",
    "\n",
    "\n",
    "* Convolutional network\n",
    "    * Full data different configurations. Kernel Sizes, Maxpool Window size, network size (convolutional and feedfoward), batch normalization, dropout, learning rate.\n",
    "    * Repeat the above tests accept optimizer and learning rate for lower number of data (Reduce from 120, to 100, 80, 60, 40, 20).\n",
    "    \n",
    "Test convolutional neural network with the kernel being used as scanning the time instances.\n",
    "    \n",
    "    \n",
    "* Recurrent network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do: \n",
    "\n",
    "Fix data provider to give:\n",
    "\n",
    "* Variable number of snippets\n",
    "* Less dimensions\n",
    "* More dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorboard command: \n",
    "tensorboard --logdir=~/Programming/Python/mlpractical/notebooks/Coursework4/logs/tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import important packages\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mlp.nn_utils as nnu\n",
    "import mlp.audio_utils as auu\n",
    "from mlp.data_providers import MSD10CustomGenreDataProvider, MSD25CustomGenreDataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to run graph\n",
    "def run_session(graph, train_step, train_data, valid_data, inputs, targets, path, summary_op, num_epoch=50): \n",
    "    \n",
    "    # Create writer files where the summaries are written\n",
    "    train_bkwriter = tf.summary.FileWriter(os.path.join('Coursework4', 'logs', 'tb_bk', path, 'train'), graph=graph)\n",
    "    valid_bkwriter = tf.summary.FileWriter(os.path.join('Coursework4', 'logs', 'tb_bk', path, 'valid'), graph=graph)\n",
    "    valid_writer = tf.summary.FileWriter(os.path.join('Coursework4', 'logs', 'tb', path, 'valid'), graph=graph, flush_secs=10)\n",
    "   \n",
    "    \n",
    "    # Intialize variables\n",
    "    with graph.as_default():       \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "    # Define session and reset variable\n",
    "    sess= tf.InteractiveSession(graph=graph)\n",
    "    sess.run(init)\n",
    "   \n",
    "    for e in range(num_epoch):\n",
    "        # Iterate through all the batches in train_data and compute\n",
    "        for b, (input_batch, target_batch) in enumerate(train_data):\n",
    "            _, summary = sess.run(\n",
    "                [train_step, summary_op], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "            train_bkwriter.add_summary(summary, e* train_data.num_batches + b)\n",
    "\n",
    "        # Run validation set every 5 epochs of the training\n",
    "        for r, (valid_input_batch, valid_target_batch) in enumerate(valid_data):\n",
    "            valid_summary = sess.run(\n",
    "                summary_op, feed_dict={inputs: valid_input_batch, targets: valid_target_batch})\n",
    "            valid_bkwriter.add_summary(valid_summary, e* train_data.num_batches + r*\n",
    "                                     (train_data.num_batches/valid_data.num_batches))\n",
    "            valid_writer.add_summary(valid_summary, e* train_data.num_batches + r*\n",
    "                                     (train_data.num_batches/valid_data.num_batches))\n",
    " \n",
    "        # Run last epoch return mean accuracy and error of last epoch\n",
    "        if e == num_epoch-1:\n",
    "            last_epoch_errors=[]\n",
    "            last_epoch_accuracies=[]\n",
    "            for r, (valid_input_batch, valid_target_batch) in enumerate(valid_data):\n",
    "                last_epoch_error, last_epoch_accuracy = sess.run(\n",
    "                    [error, accuracy], feed_dict={inputs: valid_input_batch, targets: valid_target_batch})\n",
    "                last_epoch_errors.append(last_epoch_error)\n",
    "                last_epoch_accuracies.append(last_epoch_accuracy)\n",
    "                \n",
    "    return np.mean(last_epoch_errors), np.mean(last_epoch_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 120, 25, 1)\n",
      "(10000, 120, 88, 1)\n",
      "(40000, 120, 60, 1)\n",
      "(40000, 120, 36, 1)\n",
      "(40000, 120, 12, 1)\n",
      "Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "train_data_default = MSD10CustomGenreDataProvider('train', batch_size=50, dimension='default')\n",
    "valid_data_default = MSD10CustomGenreDataProvider('valid', batch_size=50, dimension='default')\n",
    "print(train_data_default.inputs.shape)\n",
    "\n",
    "train_data_full = MSD10CustomGenreDataProvider('train', batch_size=50, dimension='full')\n",
    "valid_data_full = MSD10CustomGenreDataProvider('valid', batch_size=50, dimension='full')\n",
    "print(valid_data_full.inputs.shape)\n",
    "\n",
    "train_data_fullNoChromaDeltas = MSD10CustomGenreDataProvider('train', batch_size=50, dimension='full-nochromd')\n",
    "valid_data_fullNoChromaDeltas = MSD10CustomGenreDataProvider('valid', batch_size=50, dimension='full-nochromd')\n",
    "print(train_data_fullNoChromaDeltas.inputs.shape)\n",
    "\n",
    "train_data_timbreDeltas = MSD10CustomGenreDataProvider('train', batch_size=50, dimension='timbre-deltas')\n",
    "valid_data_timbreDeltas = MSD10CustomGenreDataProvider('valid', batch_size=50, dimension='timbre-deltas')\n",
    "print(train_data_timbreDeltas.inputs.shape)\n",
    "\n",
    "train_data_timbre = MSD10CustomGenreDataProvider('train', batch_size=50, dimension='timbre')\n",
    "valid_data_timbre = MSD10CustomGenreDataProvider('valid', batch_size=50, dimension='timbre')\n",
    "print(train_data_timbre.inputs.shape)\n",
    "\n",
    "print('Data Loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "nonLin=tf.nn.elu\n",
    "freq_vector_size = train_data.inputs.shape[2]\n",
    "print(freq_vector_size)\n",
    "\n",
    "learning_rate_test_vals = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning using Adam\n",
    "\n",
    "#### Conv 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1691.26\n",
      "Test Error: 1.2894610166549683\n",
      "Test Accuracy: 0.5478000044822693\n",
      "Runtime: 1834.28\n",
      "Test Error: 1.4058423042297363\n",
      "Test Accuracy: 0.5074000358581543\n",
      "Runtime: 2055.46\n",
      "Test Error: 1.4850925207138062\n",
      "Test Accuracy: 0.47669997811317444\n",
      "Runtime: 1788.88\n",
      "Test Error: 2.346517324447632\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 1854.12\n",
      "Test Error: 2.425968647003174\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 1870.51\n",
      "Test Error: 3.017825126647949\n",
      "Test Accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "'''Conv1D'''\n",
    "testname='conv1d_learningRate_Adam'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,20),(20,30),(30,40)]\n",
    "kPs = [(5,1),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(2,1),(2,1),(2,1),(3,1)]\n",
    "results={}\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 362.64\n",
      "Test Error: 1.629669189453125\n",
      "Test Accuracy: 0.41929998993873596\n",
      "Runtime: 348.82\n",
      "Test Error: 1.426578402519226\n",
      "Test Accuracy: 0.48879995942115784\n",
      "Runtime: 357.35\n",
      "Test Error: 1.3235278129577637\n",
      "Test Accuracy: 0.5428999662399292\n",
      "Runtime: 354.94\n",
      "Test Error: 1.6616209745407104\n",
      "Test Accuracy: 0.5085999965667725\n",
      "Runtime: 359.87\n",
      "Test Error: 1.4332122802734375\n",
      "Test Accuracy: 0.5312999486923218\n",
      "Runtime: 350.71\n",
      "Test Error: 3.235905408859253\n",
      "Test Accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_learningRate_Adam'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "kPs = [(5,freq_vector_size),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 282.29\n",
      "Test Error: 13776.8515625\n",
      "Test Accuracy: 0.10989999771118164\n",
      "Runtime: 290.53\n",
      "Test Error: 5677.1533203125\n",
      "Test Accuracy: 0.15690000355243683\n",
      "Runtime: 276.34\n",
      "Test Error: 250.16209411621094\n",
      "Test Accuracy: 0.20250000059604645\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_learningRate_Adam_nobatch'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "kPs = [(5,freq_vector_size),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for lr in [1e-6, 1e-5, 1e-4]:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = False\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 358.11\n",
      "Test Error: 1.660836935043335\n",
      "Test Accuracy: 0.398499995470047\n",
      "Runtime: 342.11\n",
      "Test Error: 2.3028151988983154\n",
      "Test Accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_learningRate_AdaDelta'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "kPs = [(5,freq_vector_size),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for lr in [1e-2, 1e-1]:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Floating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1549.1\n",
      "Test Error: 1.5013319253921509\n",
      "Test Accuracy: 0.46389999985694885\n",
      "Runtime: 1766.59\n",
      "Test Error: 1.3122329711914062\n",
      "Test Accuracy: 0.5374000072479248\n",
      "Runtime: 1729.27\n",
      "Test Error: 1.3069031238555908\n",
      "Test Accuracy: 0.5442999601364136\n",
      "Runtime: 1666.63\n",
      "Test Error: 1.4124581813812256\n",
      "Test Accuracy: 0.5465999841690063\n",
      "Runtime: 1526.94\n",
      "Test Error: 2.4137990474700928\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 1370.61\n",
      "Test Error: 3.08671236038208\n",
      "Test Accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_learningRate_Adam'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "mPPs = [(2,2),(2,1),(2,2),(3,3)]\n",
    "results={}\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 338.66\n",
      "Test Error: 2.302905797958374\n",
      "Test Accuracy: 0.09999999403953552\n",
      "Runtime: 340.98\n",
      "Test Error: 2.3043487071990967\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 389.76\n",
      "Test Error: 2.3059492111206055\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 354.9\n",
      "Test Error: 1.783705472946167\n",
      "Test Accuracy: 0.3513999879360199\n",
      "Runtime: 347.04\n",
      "Test Error: 1.5800763368606567\n",
      "Test Accuracy: 0.4358000159263611\n",
      "Runtime: 348.76\n",
      "Test Error: 1.5035948753356934\n",
      "Test Accuracy: 0.4619000256061554\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_learningRate_SGD'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "kPs = [(5,freq_vector_size),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "\n",
    "nonLin=tf.nn.elu\n",
    "\n",
    "mode_test_vals = ['default', 'timbre', 'full', 'timbre-deltas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 769.31\n",
      "Test Error: 1.3177273273468018\n",
      "Test Accuracy: 0.540399968624115\n",
      "Runtime: 438.63\n",
      "Test Error: 1.3943997621536255\n",
      "Test Accuracy: 0.5053999423980713\n",
      "Runtime: 2543.12\n",
      "Test Error: 1.265673041343689\n",
      "Test Accuracy: 0.5570999979972839\n",
      "Runtime: 1067.52\n",
      "Test Error: 1.328257441520691\n",
      "Test Accuracy: 0.5374000072479248\n"
     ]
    }
   ],
   "source": [
    "'''Conv1D'''\n",
    "testname='conv1d_data'\n",
    "lr = 1e-4\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,20),(20,30),(30,40)]\n",
    "kPs = [(5,1),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(2,1),(2,1),(2,1),(3,1)]\n",
    "results={}\n",
    "\n",
    "for mode in mode_test_vals:\n",
    "    \n",
    "    if mode=='default':\n",
    "        train_data = train_data_default\n",
    "        valid_data = valid_data_default\n",
    "    elif mode =='timbre':\n",
    "        train_data = train_data_timbre\n",
    "        valid_data = valid_data_timbre\n",
    "    elif mode=='full':\n",
    "        train_data = train_data_full\n",
    "        valid_data = valid_data_full\n",
    "    else:\n",
    "        train_data = train_data_timbreDeltas\n",
    "        valid_data = valid_data_timbreDeltas\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [mode]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[mode] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 302.77\n",
      "Test Error: 1.3035364151000977\n",
      "Test Accuracy: 0.5388999581336975\n",
      "Runtime: 281.83\n",
      "Test Error: 1.3423304557800293\n",
      "Test Accuracy: 0.5267999768257141\n",
      "Runtime: 387.9\n",
      "Test Error: 1.3324700593948364\n",
      "Test Accuracy: 0.5311999320983887\n",
      "Runtime: 315.24\n",
      "Test Error: 1.321275234222412\n",
      "Test Accuracy: 0.533799946308136\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_data'\n",
    "lr = 1e-3\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for mode in mode_test_vals:\n",
    "    \n",
    "    if mode=='default':\n",
    "        train_data = train_data_default\n",
    "        valid_data = valid_data_default\n",
    "        kPs = [(5,25),(5,1),(5,1),(5,1)]\n",
    "    elif mode =='timbre':\n",
    "        train_data = train_data_timbre\n",
    "        valid_data = valid_data_timbre\n",
    "        kPs = [(5,12),(5,1),(5,1),(5,1)]\n",
    "    elif mode=='full':\n",
    "        train_data = train_data_full\n",
    "        valid_data = valid_data_full\n",
    "        kPs = [(5,88),(5,1),(5,1),(5,1)]\n",
    "    else:\n",
    "        train_data = train_data_timbreDeltas\n",
    "        valid_data = valid_data_timbreDeltas\n",
    "        kPs = [(5,36),(5,1),(5,1),(5,1)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [mode]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[mode] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Floating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1176.37\n",
      "Test Error: 1.3052700757980347\n",
      "Test Accuracy: 0.5609999299049377\n",
      "Runtime: 753.0\n",
      "Test Error: 1.6124228239059448\n",
      "Test Accuracy: 0.5166999697685242\n",
      "Runtime: 1949.61\n",
      "Test Error: 2.347338914871216\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 1204.65\n",
      "Test Error: 2.1889312267303467\n",
      "Test Accuracy: 0.4943999946117401\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_data'\n",
    "lr = 5e-3\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "results={}\n",
    "\n",
    "for mode in mode_test_vals:\n",
    "    \n",
    "    if mode=='default':\n",
    "        train_data = train_data_default\n",
    "        valid_data = valid_data_default\n",
    "        mPPs = [(2,1),(2,1),(2,1),(3,5)]\n",
    "    elif mode =='timbre':\n",
    "        train_data = train_data_timbre\n",
    "        valid_data = valid_data_timbre\n",
    "        mPPs = [(2,1),(2,1),(2,1),(3,3)]\n",
    "    elif mode=='full':\n",
    "        train_data = train_data_full\n",
    "        valid_data = valid_data_full\n",
    "        mPPs = [(2,2),(2,2),(2,2),(3,1)]\n",
    "    else:\n",
    "        train_data = train_data_timbreDeltas\n",
    "        valid_data = valid_data_timbreDeltas\n",
    "        mPPs = [(2,1),(2,2),(2,2),(3,1)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [mode]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[mode] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kernel Size Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "\n",
    "nonLin=tf.nn.elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning using Adam\n",
    "\n",
    "#### Conv 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 2752.73\n",
      "Test Error: 1.2723042964935303\n",
      "Test Accuracy: 0.5632999539375305\n",
      "Runtime: 3615.86\n",
      "Test Error: 1.2687182426452637\n",
      "Test Accuracy: 0.5539999604225159\n",
      "Runtime: 5245.06\n",
      "Test Error: 1.2853511571884155\n",
      "Test Accuracy: 0.5538999438285828\n",
      "Runtime: 6175.62\n",
      "Test Error: 1.351770043373108\n",
      "Test Accuracy: 0.5282999277114868\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-150d4cbc71ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_fname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# <<\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mstart_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtest_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_stp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#<<\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5c7323f12bfa>\u001b[0m in \u001b[0;36mrun_session\u001b[0;34m(graph, train_step, train_data, valid_data, inputs, targets, path, summary_op, num_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m             _, summary = sess.run(\n\u001b[1;32m     22\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 feed_dict={inputs: input_batch, targets: target_batch})\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtrain_bkwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Conv1D'''\n",
    "testname='conv1d_kerSize_2'\n",
    "lr = 1e-4\n",
    "\n",
    "train_data = train_data_full\n",
    "valid_data = valid_data_full\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,20),(20,30),(30,40)]\n",
    "mPPs = [(2,1),(2,1),(2,1),(3,1)]\n",
    "results={}\n",
    "\n",
    "for ks in [15, 33, 66, 88, 100]:\n",
    "    \n",
    "    kPs = [(ks,1),(ks,1),(ks,1),(ks,1)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [ks]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[ks] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_kerSize_2'\n",
    "lr = 1e-3\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "freq_vector_size = train_data.inputs.shape[2]\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for ks in [15, 33, 66, 88, 100]:\n",
    "    \n",
    "    kPs = [(ks,freq_vector_size),(ks,1),(ks,1),(ks,1)]\n",
    "    \n",
    "    if ks==3:\n",
    "        mPPs = [(1,1),(2,1),(1,1),(59,1)]\n",
    "    elif ks==7:\n",
    "        mPPs = [(1,1),(2,1),(3,1),(19,1)]\n",
    "    elif ks==9:\n",
    "        mPPs = [(2,1),(2,1),(2,1),(14,1)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [ks]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[ks] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Floating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_kerSize'\n",
    "lr = 5e-3\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "mPPs = [(2,2),(2,1),(2,2),(3,3)]\n",
    "results={}\n",
    "\n",
    "for ks in [15, 33, 66, 88, 100]:\n",
    "    \n",
    "    kPs = [(ks,ks),(ks,ks),(ks,ks),(ks,ks)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [ks]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[ks] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Network Size Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "\n",
    "nonLin=tf.nn.elu\n",
    "\n",
    "size_tests =[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1815.94\n",
      "Test Error: 1.2429964542388916\n",
      "Test Accuracy: 0.5649999380111694\n",
      "Runtime: 1561.37\n",
      "Test Error: 1.262000560760498\n",
      "Test Accuracy: 0.5606999397277832\n",
      "Runtime: 1590.39\n",
      "Test Error: 1.2952401638031006\n",
      "Test Accuracy: 0.5453000068664551\n",
      "Runtime: 1470.65\n",
      "Test Error: 1.3059223890304565\n",
      "Test Accuracy: 0.5467000007629395\n"
     ]
    }
   ],
   "source": [
    "'''Conv1D'''\n",
    "testname='conv1d_NetSize'\n",
    "lr = 1e-4\n",
    "\n",
    "train_data = train_data_full\n",
    "valid_data = valid_data_full\n",
    "results={}\n",
    "\n",
    "size_tests = [1,2,3,4]\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "mPPs = [(2,1),(2,1),(2,1),(3,1)]\n",
    "kPs = [(3,1),(3,1),(3,1),(3,1)]\n",
    "\n",
    "for st in size_tests:\n",
    "    \n",
    "    if st==1: \n",
    "        cnPs = [(1,5),(5,10),(10,20),(20,30)]\n",
    "    elif st==2:\n",
    "        cnPs = [(1,2),(2,4),(4,8),(8,16)]\n",
    "    elif st==3:\n",
    "        cnPs = [(1,2),(2,3),(3,6),(6,9)]\n",
    "    else:\n",
    "        cnPs = [(1,2),(2,3),(3,4),(4,5)]\n",
    "\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [st]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[st] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 586.7\n",
      "Test Error: 1.3293771743774414\n",
      "Test Accuracy: 0.5414999723434448\n",
      "Runtime: 283.62\n",
      "Test Error: 1.4303994178771973\n",
      "Test Accuracy: 0.4883999526500702\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_NetSize'\n",
    "lr = 1e-3\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "freq_vector_size = train_data.inputs.shape[2]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "kPs = [(3,freq_vector_size),(3,1),(3,1),(3,1)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "    \n",
    "for st in size_tests:\n",
    "    \n",
    "    if st==1: \n",
    "        cnPs = [(1,20),(20,100),(100,200),(200,400)]\n",
    "        \n",
    "    else:\n",
    "        cnPs = [(1,5),(5,10),(10,20),(20,200)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [st]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[st] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Floating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 2641.67\n",
      "Test Error: 2.3880724906921387\n",
      "Test Accuracy: 0.10000000149011612\n",
      "Runtime: 901.95\n",
      "Test Error: 1.5716500282287598\n",
      "Test Accuracy: 0.5067999958992004\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_NetSize'\n",
    "lr = 5e-3\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "mPPs = [(2,2),(2,1),(2,2),(3,3)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "results={}\n",
    "    \n",
    "for st in size_tests:\n",
    "    \n",
    "    if st==1: \n",
    "        cnPs = [(1,20),(20,50),(50,100),(100,200)]\n",
    "    else:\n",
    "        cnPs = [(1,5),(5,10),(10,15),(15,20)]\n",
    "    \n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [st]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[st] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25-Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 120, 25, 1)\n",
      "(10000, 120, 88, 1)\n",
      "(40000, 120, 60, 1)\n",
      "(40000, 120, 36, 1)\n",
      "(40000, 120, 12, 1)\n",
      "Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "train_data_default25 = MSD25CustomGenreDataProvider('train', batch_size=50, dimension='default')\n",
    "valid_data_default25 = MSD25CustomGenreDataProvider('valid', batch_size=50, dimension='default')\n",
    "print(train_data_default25.inputs.shape)\n",
    "\n",
    "train_data_full25 = MSD25CustomGenreDataProvider('train', batch_size=50, dimension='full')\n",
    "valid_data_full25 = MSD25CustomGenreDataProvider('valid', batch_size=50, dimension='full')\n",
    "print(valid_data_full25.inputs.shape)\n",
    "\n",
    "train_data_fullNoChromaDeltas25 = MSD25CustomGenreDataProvider('train', batch_size=50, dimension='full-nochromd')\n",
    "valid_data_fullNoChromaDeltas25 = MSD25CustomGenreDataProvider('valid', batch_size=50, dimension='full-nochromd')\n",
    "print(train_data_fullNoChromaDeltas25.inputs.shape)\n",
    "\n",
    "train_data_timbreDeltas25 = MSD25CustomGenreDataProvider('train', batch_size=50, dimension='timbre-deltas')\n",
    "valid_data_timbreDeltas25 = MSD25CustomGenreDataProvider('valid', batch_size=50, dimension='timbre-deltas')\n",
    "print(train_data_timbreDeltas25.inputs.shape)\n",
    "\n",
    "train_data_timbre25 = MSD25CustomGenreDataProvider('train', batch_size=50, dimension='timbre')\n",
    "valid_data_timbre25 = MSD25CustomGenreDataProvider('valid', batch_size=50, dimension='timbre')\n",
    "print(train_data_timbre25.inputs.shape)\n",
    "\n",
    "print('Data Loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "train_data = train_data_fullNoChromaDeltas25\n",
    "valid_data = valid_data_fullNoChromaDeltas25\n",
    "\n",
    "nonLin=tf.nn.elu\n",
    "freq_vector_size = train_data.inputs.shape[2]\n",
    "print(freq_vector_size)\n",
    "\n",
    "learning_rate_test_vals = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning using Adam\n",
    "\n",
    "#### Conv 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1690.69\n",
      "Test Error: 2.5420641899108887\n",
      "Test Accuracy: 0.23680000007152557\n",
      "Runtime: 1723.81\n",
      "Test Error: 2.86385440826416\n",
      "Test Accuracy: 0.10700000822544098\n",
      "Runtime: 1730.56\n",
      "Test Error: 2.897284507751465\n",
      "Test Accuracy: 0.10179999470710754\n",
      "Runtime: 1550.25\n",
      "Test Error: 3.3122968673706055\n",
      "Test Accuracy: 0.03999999910593033\n",
      "Runtime: 1551.59\n",
      "Test Error: 3.543579339981079\n",
      "Test Accuracy: 0.03999999910593033\n",
      "Runtime: 1562.55\n",
      "Test Error: 4.473163604736328\n",
      "Test Accuracy: 0.03999999910593033\n"
     ]
    }
   ],
   "source": [
    "'''Conv1D'''\n",
    "testname='conv1d_learningRate_Adam_25'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,20),(20,30),(30,40)]\n",
    "kPs = [(5,1),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(2,1),(2,1),(2,1),(3,1)]\n",
    "results={}\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 348.31\n",
      "Test Error: 2.7217133045196533\n",
      "Test Accuracy: 0.1779000163078308\n",
      "Runtime: 355.27\n",
      "Test Error: 2.534174919128418\n",
      "Test Accuracy: 0.23319999873638153\n",
      "Runtime: 359.66\n",
      "Test Error: 2.5458245277404785\n",
      "Test Accuracy: 0.23099999129772186\n",
      "Runtime: 356.04\n",
      "Test Error: 2.5600829124450684\n",
      "Test Accuracy: 0.2363000065088272\n",
      "Runtime: 368.33\n",
      "Test Error: 2.5466785430908203\n",
      "Test Accuracy: 0.23170000314712524\n",
      "Runtime: 382.41\n",
      "Test Error: 4.5097336769104\n",
      "Test Accuracy: 0.03999999910593033\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_learningRate_Adam_25'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "kPs = [(5,freq_vector_size),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Floating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1481.8\n",
      "Test Error: 2.679424524307251\n",
      "Test Accuracy: 0.19680000841617584\n",
      "Runtime: 1482.43\n",
      "Test Error: 2.553729295730591\n",
      "Test Accuracy: 0.2224999964237213\n",
      "Runtime: 1504.71\n",
      "Test Error: 2.5977416038513184\n",
      "Test Accuracy: 0.1962999850511551\n",
      "Runtime: 1496.81\n",
      "Test Error: 2.5554988384246826\n",
      "Test Accuracy: 0.23489999771118164\n",
      "Runtime: 1350.87\n",
      "Test Error: 3.543579339981079\n",
      "Test Accuracy: 0.03999999910593033\n",
      "Runtime: 1303.88\n",
      "Test Error: 5.572109222412109\n",
      "Test Accuracy: 0.03999999910593033\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_learningRate_Adam_25'\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "mPPs = [(2,2),(2,1),(2,2),(3,3)]\n",
    "results={}\n",
    "\n",
    "for lr in learning_rate_test_vals:\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [lr, 'batch']) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[str(lr)] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "\n",
    "nonLin=tf.nn.elu\n",
    "\n",
    "mode_test_vals = ['default', 'timbre', 'full', 'timbre-deltas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 790.73\n",
      "Test Error: 2.518949508666992\n",
      "Test Accuracy: 0.24699999392032623\n",
      "Runtime: 440.23\n",
      "Test Error: 2.5674610137939453\n",
      "Test Accuracy: 0.22710001468658447\n",
      "Runtime: 2518.65\n",
      "Test Error: 2.505674123764038\n",
      "Test Accuracy: 0.24089999496936798\n",
      "Runtime: 1093.4\n",
      "Test Error: 2.5510127544403076\n",
      "Test Accuracy: 0.2329999953508377\n"
     ]
    }
   ],
   "source": [
    "'''Conv1D'''\n",
    "testname='conv1d_data_25'\n",
    "lr = 1e-4\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,20),(20,30),(30,40)]\n",
    "kPs = [(5,1),(5,1),(5,1),(5,1)]\n",
    "mPPs = [(2,1),(2,1),(2,1),(3,1)]\n",
    "results={}\n",
    "\n",
    "for mode in mode_test_vals:\n",
    "    \n",
    "    if mode=='default':\n",
    "        train_data = train_data_default25\n",
    "        valid_data = valid_data_default25\n",
    "    elif mode =='timbre':\n",
    "        train_data = train_data_timbre25\n",
    "        valid_data = valid_data_timbre25\n",
    "    elif mode=='full':\n",
    "        train_data = train_data_full25\n",
    "        valid_data = valid_data_full25\n",
    "    else:\n",
    "        train_data = train_data_timbreDeltas25\n",
    "        valid_data = valid_data_timbreDeltas25\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [mode]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[mode] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con 2D Full Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 307.84\n",
      "Test Error: 2.6351494789123535\n",
      "Test Accuracy: 0.2247999906539917\n",
      "Runtime: 286.05\n",
      "Test Error: 2.6063339710235596\n",
      "Test Accuracy: 0.23430000245571136\n",
      "Runtime: 430.15\n",
      "Test Error: 2.521533727645874\n",
      "Test Accuracy: 0.2328999936580658\n",
      "Runtime: 316.0\n",
      "Test Error: 2.590006113052368\n",
      "Test Accuracy: 0.2363000065088272\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D Full Frequency'''\n",
    "testname='conv2dFF_data_25'\n",
    "lr = 5e-3\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,50),(50,100),(100,200)]\n",
    "mPPs = [(1,1),(2,1),(2,1),(29,1)]\n",
    "results={}\n",
    "\n",
    "padding=['VALID', 'SAME', 'SAME', 'SAME']\n",
    "\n",
    "for mode in mode_test_vals:\n",
    "    \n",
    "    if mode=='default':\n",
    "        train_data = train_data_default25\n",
    "        valid_data = valid_data_default25\n",
    "        kPs = [(5,25),(5,1),(5,1),(5,1)]\n",
    "    elif mode =='timbre':\n",
    "        train_data = train_data_timbre25\n",
    "        valid_data = valid_data_timbre25\n",
    "        kPs = [(5,12),(5,1),(5,1),(5,1)]\n",
    "    elif mode=='full':\n",
    "        train_data = train_data_full25\n",
    "        valid_data = valid_data_full25\n",
    "        kPs = [(5,88),(5,1),(5,1),(5,1)]\n",
    "    else:\n",
    "        train_data = train_data_timbreDeltas25\n",
    "        valid_data = valid_data_timbreDeltas25\n",
    "        kPs = [(5,36),(5,1),(5,1),(5,1)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN, convPadding=padding)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [mode]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[mode] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv 2D Floating Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 1139.28\n",
      "Test Error: 2.8131134510040283\n",
      "Test Accuracy: 0.2263999879360199\n",
      "Runtime: 669.97\n",
      "Test Error: 3.221024751663208\n",
      "Test Accuracy: 0.20319999754428864\n",
      "Runtime: 1908.85\n",
      "Test Error: 3.3543500900268555\n",
      "Test Accuracy: 0.03999999910593033\n",
      "Runtime: 1211.02\n",
      "Test Error: 3.858874559402466\n",
      "Test Accuracy: 0.19670000672340393\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_data_25'\n",
    "lr = 5e-3\n",
    "\n",
    "fcPs = [(1500,200),(200,train_data.num_classes)]\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "results={}\n",
    "\n",
    "for mode in mode_test_vals:\n",
    "    \n",
    "    if mode=='default':\n",
    "        train_data = train_data_default25\n",
    "        valid_data = valid_data_default25\n",
    "        mPPs = [(2,1),(2,1),(2,1),(3,5)]\n",
    "    elif mode =='timbre':\n",
    "        train_data = train_data_timbre25\n",
    "        valid_data = valid_data_timbre25\n",
    "        mPPs = [(2,1),(2,1),(2,1),(3,3)]\n",
    "    elif mode=='full':\n",
    "        train_data = train_data_full25\n",
    "        valid_data = valid_data_full25\n",
    "        mPPs = [(2,2),(2,2),(2,2),(3,1)]\n",
    "    else:\n",
    "        train_data = train_data_timbreDeltas25\n",
    "        valid_data = valid_data_timbreDeltas25\n",
    "        mPPs = [(2,1),(2,2),(2,2),(3,1)]\n",
    "    \n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    [layer_outs, error, accuracy, inputs, targets, sop, t_stp] = nnu.net_model(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs, cnNonLinearity=nonLin,\n",
    "                            fcParams=fcPs, fcNonLinearity=nonLin, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [mode]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[mode] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_crnn(graph, train_data, convParams=None, cnStrides=1, rcParams=None,\n",
    "            kernelParams=None, maxPoolParams=None, nonL=tf.nn.elu, dropPFc=1, \n",
    "            dropPCn=1, bnFc=False, bnCn=False, \n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=5e-6),\n",
    "            convPadding='SAME'):\n",
    "    \n",
    "    with graph.as_default():\n",
    "        \n",
    "        layer_outs = []\n",
    "        \n",
    "        with tf.name_scope('data'):\n",
    "            targets=tf.placeholder(tf.float32, [None, train_data.num_classes], name='targets')\n",
    "            inputs=tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2], train_data.inputs.shape[3]], name='inputs')\n",
    "            \n",
    "        conv_outs = nnu.multi_convLayers(inputs, convParams=convParams, cnStrides=cnStrides, kernelParams=kernelParams, maxPoolParams=maxPoolParams,\n",
    "                                            cnNonLinearity=nonL, dropPCn=dropPCn, bnCn=bnCn, padding=convPadding)\n",
    "            \n",
    "        layer_outs.extend(conv_outs)\n",
    "        \n",
    "        \"\"\"\n",
    "        Recurrent Layer\n",
    "        \"\"\"\n",
    "        \n",
    "        recurrent_inputs = tf.squeeze(layer_outs[-1], axis=2)\n",
    "        print(layer_outs[-1].get_shape)\n",
    "        print(recurrent_inputs.get_shape()[2])\n",
    "        \n",
    "        \n",
    "        weights_in = tf.Variable(tf.random_normal([recurrent_inputs.get_shape().as_list()[2], rcParams[0]]))\n",
    "        weights_out = tf.Variable(tf.random_normal([rcParams[0], train_data.num_classes]))\n",
    "        \n",
    "        biases_in = tf.Variable(tf.zeros([rcParams[0], ])) #<< what is the space\n",
    "        biases_out = tf.Variable(tf.zeros([train_data.num_classes, ]))\n",
    "        \n",
    "        inputs_in = tf.reshape(recurrent_inputs, [-1, recurrent_inputs.get_shape().as_list()[-1]])\n",
    "        rnn_in = tf.matmul(inputs_in, weights_in) + biases_in\n",
    "        \n",
    "        rnn_in = tf.reshape(rnn_in, [-1, recurrent_inputs.get_shape().as_list()[1], rcParams[0]])\n",
    "        \n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(rcParams[0], forget_bias=1.0, state_is_tuple=True)\n",
    "        initial_state = lstm_cell.zero_state(50, dtype=tf.float32) #<< batch size = 50\n",
    "        \n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, rnn_in, initial_state =initial_state,\n",
    "                                                time_major=False)\n",
    "        \n",
    "        outputs = tf.unpack(tf.transpose(outputs, [1, 0, 2]))\n",
    "        prediction = tf.matmul(outputs[-1], weights_out) + biases_out\n",
    "        \n",
    "        with tf.name_scope('error'):\n",
    "            error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, targets))\n",
    "            \n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(\n",
    "                tf.equal(tf.argmax(prediction, 1), tf.argmax(targets, 1)), \n",
    "                tf.float32))\n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            gvs = optimizer.compute_gradients(error)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            train_step = optimizer.apply_gradients(capped_gvs)\n",
    "            \n",
    "        summary_merged = nnu.define_summaries(scalar_sd={'error': error, 'accuracy': accuracy})        \n",
    "    \n",
    "    return layer_outs, prediction, error, accuracy, inputs, targets, summary_merged, train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 120, 60, 1)\n",
      "(40000, 120, 60, 1)\n",
      "Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "train_data_fullNoChromaDeltas = MSD10CustomGenreDataProvider('train', batch_size=50, dimension='full-nochromd')\n",
    "valid_data_fullNoChromaDeltas = MSD10CustomGenreDataProvider('valid', batch_size=50, dimension='full-nochromd')\n",
    "print(train_data_fullNoChromaDeltas.inputs.shape)\n",
    "\n",
    "train_data_fullNoChromaDeltas25 = MSD25CustomGenreDataProvider('train', batch_size=50, dimension='full-nochromd')\n",
    "valid_data_fullNoChromaDeltas25 = MSD25CustomGenreDataProvider('valid', batch_size=50, dimension='full-nochromd')\n",
    "print(train_data_fullNoChromaDeltas25.inputs.shape)\n",
    "\n",
    "print('Data Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1510.3\n",
      "Test Error: 1.3141944408416748\n",
      "Test Accuracy: 0.5483999848365784\n",
      "[35]\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1512.52\n",
      "Test Error: 1.3160910606384277\n",
      "Test Accuracy: 0.5397999286651611\n",
      "[50]\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1508.99\n",
      "Test Error: 1.5288825035095215\n",
      "Test Accuracy: 0.4564000070095062\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_crnn'\n",
    "lr = 5e-3\n",
    "epochs=20\n",
    "rnn_hidden_layers = [20, 35, 50]\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "mPPs = [(2,2),(2,2),(1,5),(1,3)]\n",
    "results={}\n",
    "\n",
    "for rc in rnn_hidden_layers:\n",
    "    \n",
    "    rcPs = [rc]\n",
    "    print(rcPs)\n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    \n",
    "    [layer_outs, prediction, error, accuracy, inputs, targets, sop, t_stp] = net_crnn(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs,\n",
    "                            rcParams=rcPs, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [rc]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[rc] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1523.81\n",
      "Test Error: 2.554560899734497\n",
      "Test Accuracy: 0.21880000829696655\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1521.71\n",
      "Test Error: 2.569983720779419\n",
      "Test Accuracy: 0.21570000052452087\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1477.06\n",
      "Test Error: 3.2230424880981445\n",
      "Test Accuracy: 0.03999999538064003\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_crnn_25'\n",
    "lr = 5e-3\n",
    "epochs=20\n",
    "rnn_hidden_layers = [20, 35, 50]\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas25\n",
    "valid_data = valid_data_fullNoChromaDeltas25\n",
    "\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "mPPs = [(2,2),(2,2),(1,5),(1,3)]\n",
    "results={}\n",
    "\n",
    "for rc in rnn_hidden_layers:\n",
    "    \n",
    "    rcPs = [rc]\n",
    "    # Test paramter\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    btcN = True\n",
    "    \n",
    "    #Define graph\n",
    "    TGraph=tf.Graph()\n",
    "    \n",
    "    [layer_outs, prediction, error, accuracy, inputs, targets, sop, t_stp] = net_crnn(TGraph, train_data, \n",
    "                            convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs,\n",
    "                            rcParams=rcPs, optimizer=optimizer, bnCn=btcN)\n",
    "\n",
    "    \n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [rc]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[rc] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1640.67\n",
      "Test Error: 2.304070472717285\n",
      "Test Accuracy: 0.10000000149011612\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1666.47\n",
      "Test Error: 2.3041648864746094\n",
      "Test Accuracy: 0.10000000149011612\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1745.21\n",
      "Test Error: 1.4133715629577637\n",
      "Test Accuracy: 0.5031999945640564\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1494.36\n",
      "Test Error: 1.3425441980361938\n",
      "Test Accuracy: 0.5275999903678894\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1690.42\n",
      "Test Error: 2.304098129272461\n",
      "Test Accuracy: 0.10000000149011612\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1713.69\n",
      "Test Error: 2.3040771484375\n",
      "Test Accuracy: 0.10000000149011612\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1715.39\n",
      "Test Error: 2.309490919113159\n",
      "Test Accuracy: 0.10000000149011612\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1504.91\n",
      "Test Error: 1.3119723796844482\n",
      "Test Accuracy: 0.538599967956543\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1628.83\n",
      "Test Error: 2.3074593544006348\n",
      "Test Accuracy: 0.10000000149011612\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1631.83\n",
      "Test Error: 2.3397321701049805\n",
      "Test Accuracy: 0.09999999403953552\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1640.3\n",
      "Test Error: 2.333505868911743\n",
      "Test Accuracy: 0.09999999403953552\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'mp-layer-3/MaxPool:0' shape=(?, 30, 1, 70) dtype=float32>>\n",
      "70\n",
      "Runtime: 1411.72\n",
      "Test Error: 2.3088748455047607\n",
      "Test Accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_crnn_learning_rate_dropout'\n",
    "epochs=20\n",
    "rnn_hidden_layers = [20]\n",
    "\n",
    "learning_rate_test = [5e-3, 1e-2, 5e-2]\n",
    "dropout_test = [0.4, 0.6, 0.8, 1]\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    " \n",
    "rcPs = rnn_hidden_layers\n",
    "cnPs = [(1,10),(10,30),(30,50),(50,70)]\n",
    "kPs = [(3,3),(3,3),(3,3),(3,3)]\n",
    "mPPs = [(2,2),(2,2),(1,5),(1,3)]\n",
    "results={}\n",
    "\n",
    "for lr in learning_rate_test:\n",
    "    for drp in dropout_test:\n",
    "    \n",
    "        # Test paramter\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        btcN = True\n",
    "\n",
    "        #Define graph\n",
    "        TGraph=tf.Graph()\n",
    "\n",
    "        [layer_outs, prediction, error, accuracy, inputs, targets, sop, t_stp] = net_crnn(TGraph, train_data, \n",
    "                                convParams=cnPs, maxPoolParams=mPPs, kernelParams=kPs,\n",
    "                                rcParams=rcPs, optimizer=optimizer, bnCn=btcN, dropPCn=drp)\n",
    "\n",
    "\n",
    "        # Perform test and save results in dictionary\n",
    "        path = auu.create_fname(testname, [lr, drp]) # <<\n",
    "        start_run = time.time()\n",
    "        test_error, test_accuracy = run_session(TGraph, t_stp, train_data, valid_data, inputs, targets, path, sop, epochs)\n",
    "        runtime = round(time.time() - start_run, 2)\n",
    "        \n",
    "        name = str(lr)+'_'+str(drp)\n",
    "        results[name] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "        # Print results to console\n",
    "        print(\"Runtime: {0}\".format(runtime))\n",
    "        print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MultiKernel CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell with Function that produces a visual representation of the graph passed using TENSORBOARD.\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import datetime\n",
    "\n",
    "def show_graph(graph_def, frame_size=(900, 600)):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:{height}px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(height=frame_size[1], data=repr(str(graph_def)), id='graph'+timestamp)\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:{width}px;height:{height}px;border:0\" srcdoc=\"{src}\"></iframe>\n",
    "    \"\"\".format(width=frame_size[0], height=frame_size[1] + 20, src=code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 120, 60, 1]\n",
      "[50, 15, 12, 40]\n",
      "(7200, 4500)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 12, 40), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 7200), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 4500), dtype=float32)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[7200,4500]\n\t [[Node: data/fc-layer-0/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](data/fc-layer-0/truncated_normal/shape)]]\n\nCaused by op 'data/fc-layer-0/truncated_normal/TruncatedNormal', defined at:\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-f8d8ff223a23>\", line 81, in <module>\n    fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n  File \"c:\\users\\mark\\programming\\python\\mlpractical\\mlp\\nn_utils.py\", line 88, in multi_fcLayers\n    output = fully_connected_layer(data_in, fcParam, nonlinearity=fcNonLinearity[fcCount], batchNorm=bnFc, dropProb=dropPFc)\n  File \"c:\\users\\mark\\programming\\python\\mlpractical\\mlp\\nn_utils.py\", line 43, in fully_connected_layer\n    stddev=2. / (layerParams[0]+layerParams[1])**0.5), name='weights')\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 174, in truncated_normal\n    seed2=seed2)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 293, in _truncated_normal\n    seed=seed, seed2=seed2, name=name)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[7200,4500]\n\t [[Node: data/fc-layer-0/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](data/fc-layer-0/truncated_normal/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[7200,4500]\n\t [[Node: data/fc-layer-0/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](data/fc-layer-0/truncated_normal/shape)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f8d8ff223a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_fname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mk1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# <<\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mstart_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mtest_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTGraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_merged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mk1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mruntime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#<<\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5c7323f12bfa>\u001b[0m in \u001b[0;36mrun_session\u001b[0;34m(graph, train_step, train_data, valid_data, inputs, targets, path, summary_op, num_epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[1;31m# Define session and reset variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[7200,4500]\n\t [[Node: data/fc-layer-0/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](data/fc-layer-0/truncated_normal/shape)]]\n\nCaused by op 'data/fc-layer-0/truncated_normal/TruncatedNormal', defined at:\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-f8d8ff223a23>\", line 81, in <module>\n    fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n  File \"c:\\users\\mark\\programming\\python\\mlpractical\\mlp\\nn_utils.py\", line 88, in multi_fcLayers\n    output = fully_connected_layer(data_in, fcParam, nonlinearity=fcNonLinearity[fcCount], batchNorm=bnFc, dropProb=dropPFc)\n  File \"c:\\users\\mark\\programming\\python\\mlpractical\\mlp\\nn_utils.py\", line 43, in fully_connected_layer\n    stddev=2. / (layerParams[0]+layerParams[1])**0.5), name='weights')\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py\", line 174, in truncated_normal\n    seed2=seed2)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_random_ops.py\", line 293, in _truncated_normal\n    seed=seed, seed2=seed2, name=name)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[7200,4500]\n\t [[Node: data/fc-layer-0/truncated_normal/TruncatedNormal = TruncatedNormal[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](data/fc-layer-0/truncated_normal/shape)]]\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_multikernel'\n",
    "epochs=20\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "fcParams = [(1500,1000),(1000,train_data.num_classes)]\n",
    "cnPs1 = [(1,5),(5,10)]\n",
    "cnPs2 = [(1,5),(5,10)]\n",
    "cnPs3 = [(1,5),(5,10)]\n",
    "cnPs4 = [(1,5),(5,10)]\n",
    "cnPsO = [(40,50)]\n",
    "\n",
    "kPs = [(5,1),(5,1)]\n",
    "kPsO = [(5,1)]\n",
    "mPPs = [(2,1),(2,1)]\n",
    "mPPsO = [(2,1)]\n",
    "results={}\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "btcN = True\n",
    "TGraph=tf.Graph()    \n",
    "    \n",
    "with TGraph.as_default():\n",
    "    layer_outs = []\n",
    "        \n",
    "    '''\n",
    "    Set placeholder of targets and inputs.\n",
    "    The inputs placeholder dimensions will be determined by the data.\n",
    "    '''\n",
    "    with tf.name_scope('data'):\n",
    "        targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "        inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2],\n",
    "                                                 train_data.inputs.shape[3]], name='inputs')\n",
    "        \n",
    "        input_shape= inputs.get_shape().as_list()\n",
    "        print(input_shape)\n",
    "        inputs1 = tf.slice(inputs, [0,0,0,0], [50, input_shape[1], 12, input_shape[3]]) #timbre\n",
    "        inputs2 = tf.slice(inputs, [0,0,15,0], [50, input_shape[1], 12, input_shape[3]]) #deltas\n",
    "        inputs3 = tf.slice(inputs, [0,0,30,0], [50, input_shape[1], 12, input_shape[3]]) #deltadeltas\n",
    "        inputs4 = tf.slice(inputs, [0,0,45,0], [50, input_shape[1], 12, input_shape[3]]) #chroma\n",
    "        \n",
    "        '''\n",
    "        Create Convolutional Layers\n",
    "        '''\n",
    "        conv_outs1 = nnu.multi_convLayers(inputs1, convParams=cnPs1, cnStrides=1, kernelParams=kPs, \n",
    "                                        maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                        bnCn=btcN, padding='SAME')\n",
    "            \n",
    "        conv_outs2 = nnu.multi_convLayers(inputs2, convParams=cnPs2, cnStrides=1, kernelParams=kPs, \n",
    "                                        maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                        bnCn=btcN, padding='SAME')\n",
    "        \n",
    "        conv_outs3 = nnu.multi_convLayers(inputs3, convParams=cnPs3, cnStrides=1, kernelParams=kPs, \n",
    "                                        maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                        bnCn=btcN, padding='SAME')   \n",
    "            \n",
    "        conv_outs4 = nnu.multi_convLayers(inputs4, convParams=cnPs4, cnStrides=1, kernelParams=kPs, \n",
    "                                        maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                        bnCn=btcN, padding='SAME')      \n",
    "        \n",
    "        \n",
    "        ensemble_out = tf.concat(3, [conv_outs1[-1], conv_outs2[-1]])\n",
    "        ensemble_out = tf.concat(3, [ensemble_out, conv_outs3[-1]])\n",
    "        ensemble_out = tf.concat(3, [ensemble_out, conv_outs4[-1]])\n",
    "            \n",
    "        conv_outsO = nnu.multi_convLayers(ensemble_out, convParams=cnPsO, cnStrides=1, kernelParams=kPsO, \n",
    "                                        maxPoolParams=mPPsO, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                        bnCn=btcN, padding='SAME')  \n",
    "        \n",
    "        shape= conv_outsO[-1].get_shape().as_list()\n",
    "        print(shape)\n",
    "        fcParams[0] = (shape[1]*shape[2]*shape[3], fcParams[0][1])\n",
    "        print(fcParams[0])\n",
    "        print(conv_outsO[-1])\n",
    "        input_data = tf.reshape(conv_outsO[-1], [-1, fcParams[0][0]])\n",
    "        print(input_data)\n",
    "        \n",
    "        fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n",
    "        print(fc_outs[-1])\n",
    "        layer_outs.extend(fc_outs)\n",
    "        input_data=layer_outs[-1]\n",
    "        \n",
    "        with tf.name_scope('output-layer'):\n",
    "            output = nnu.fully_connected_layer(input_data, fcParams[-1], nonlinearity=tf.identity)\n",
    "            layer_outs.append(output)\n",
    "            \n",
    "                            \n",
    "        # Define the operations for the error\n",
    "        with tf.name_scope('error'):   \n",
    "            error = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(layer_outs[-1], targets))\n",
    "\n",
    "        # Define the function to calculate the accuracy of the system\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(\n",
    "                tf.equal(tf.argmax(layer_outs[-1], 1), tf.argmax(targets, 1)), \n",
    "                tf.float32))\n",
    "        \n",
    "        # Define optimizer and train step\n",
    "        with tf.name_scope('train'):\n",
    "            gvs = optimizer.compute_gradients(error)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n",
    "            train_step = optimizer.apply_gradients(capped_gvs)\n",
    "                \n",
    "        # Define summaries        \n",
    "        summary_merged = nnu.define_summaries(scalar_sd={'error': error, 'accuracy': accuracy})        \n",
    "        \n",
    "#show_graph(TGraph)        \n",
    "\n",
    "\n",
    "# Perform test and save results in dictionary\n",
    "path = auu.create_fname(testname, ['mk1']) # <<\n",
    "start_run = time.time()\n",
    "test_error, test_accuracy = run_session(TGraph, train_step, train_data, valid_data, inputs, targets, path, summary_merged, epochs)\n",
    "runtime = round(time.time() - start_run, 2)\n",
    "results['mk1'] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "# Print results to console\n",
    "print(\"Runtime: {0}\".format(runtime))\n",
    "print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 120, 60, 1]\n",
      "[50, 15, 6, 50]\n",
      "(4500, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 6, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 4500), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 659.25\n",
      "Test Error: 1.4037420749664307\n",
      "Test Accuracy: 0.5006999969482422\n",
      "[None, 120, 60, 1]\n",
      "[50, 15, 8, 50]\n",
      "(6000, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 8, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 6000), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 823.45\n",
      "Test Error: 1.3438912630081177\n",
      "Test Accuracy: 0.521899938583374\n",
      "[None, 120, 60, 1]\n",
      "[50, 15, 10, 50]\n",
      "(7500, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 10, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 7500), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 1005.11\n",
      "Test Error: 1.3134313821792603\n",
      "Test Accuracy: 0.5399999618530273\n",
      "[None, 120, 60, 1]\n",
      "[50, 15, 12, 50]\n",
      "(9000, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 12, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 9000), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 1127.5\n",
      "Test Error: 1.2954041957855225\n",
      "Test Accuracy: 0.5478999614715576\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv1d_multikernel_2'\n",
    "epochs=20\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "fcParams = [(1500,1000),(1000,train_data.num_classes)]\n",
    "cnPs1 = [(1,5),(5,10)]\n",
    "cnPs2 = [(1,5),(5,10)]\n",
    "cnPs3 = [(1,5),(5,10)]\n",
    "cnPs4 = [(1,5),(5,10)]\n",
    "cnPsO = [(40,50)]\n",
    "\n",
    "kPs = [(5,1),(5,1)]\n",
    "kPsO = [(5,1)]\n",
    "mPPs = [(2,1),(2,1)]\n",
    "mPPsO = [(2,1)]\n",
    "results={}\n",
    "\n",
    "\n",
    "for sz in [6,8,10,12]:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    btcN = True\n",
    "    TGraph=tf.Graph()    \n",
    "\n",
    "    with TGraph.as_default():\n",
    "        layer_outs = []\n",
    "\n",
    "        '''\n",
    "        Set placeholder of targets and inputs.\n",
    "        The inputs placeholder dimensions will be determined by the data.\n",
    "        '''\n",
    "        with tf.name_scope('data'):\n",
    "            targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "            inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2],\n",
    "                                                     train_data.inputs.shape[3]], name='inputs')\n",
    "\n",
    "            input_shape= inputs.get_shape().as_list()\n",
    "            print(input_shape)\n",
    "            inputs1 = tf.slice(inputs, [0,0,0,0], [50, input_shape[1], sz, input_shape[3]]) #timbre\n",
    "            inputs2 = tf.slice(inputs, [0,0,15,0], [50, input_shape[1], sz, input_shape[3]]) #deltas\n",
    "            inputs3 = tf.slice(inputs, [0,0,30,0], [50, input_shape[1], sz, input_shape[3]]) #deltadeltas\n",
    "            inputs4 = tf.slice(inputs, [0,0,45,0], [50, input_shape[1], sz, input_shape[3]]) #chroma\n",
    "\n",
    "            '''\n",
    "            Create Convolutional Layers\n",
    "            '''\n",
    "            conv_outs1 = nnu.multi_convLayers(inputs1, convParams=cnPs1, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')\n",
    "\n",
    "            conv_outs2 = nnu.multi_convLayers(inputs2, convParams=cnPs2, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')\n",
    "\n",
    "            conv_outs3 = nnu.multi_convLayers(inputs3, convParams=cnPs3, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')   \n",
    "\n",
    "            conv_outs4 = nnu.multi_convLayers(inputs4, convParams=cnPs4, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')      \n",
    "\n",
    "\n",
    "            ensemble_out = tf.concat(3, [conv_outs1[-1], conv_outs2[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs3[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs4[-1]])\n",
    "\n",
    "            conv_outsO = nnu.multi_convLayers(ensemble_out, convParams=cnPsO, cnStrides=1, kernelParams=kPsO, \n",
    "                                            maxPoolParams=mPPsO, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')  \n",
    "\n",
    "            shape= conv_outsO[-1].get_shape().as_list()\n",
    "            print(shape)\n",
    "            fcParams[0] = (shape[1]*shape[2]*shape[3], fcParams[0][1])\n",
    "            print(fcParams[0])\n",
    "            print(conv_outsO[-1])\n",
    "            input_data = tf.reshape(conv_outsO[-1], [-1, fcParams[0][0]])\n",
    "            print(input_data)\n",
    "\n",
    "            fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n",
    "            print(fc_outs[-1])\n",
    "            layer_outs.extend(fc_outs)\n",
    "            input_data=layer_outs[-1]\n",
    "\n",
    "            with tf.name_scope('output-layer'):\n",
    "                output = nnu.fully_connected_layer(input_data, fcParams[-1], nonlinearity=tf.identity)\n",
    "                layer_outs.append(output)\n",
    "\n",
    "\n",
    "            # Define the operations for the error\n",
    "            with tf.name_scope('error'):   \n",
    "                error = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(layer_outs[-1], targets))\n",
    "\n",
    "            # Define the function to calculate the accuracy of the system\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(\n",
    "                    tf.equal(tf.argmax(layer_outs[-1], 1), tf.argmax(targets, 1)), \n",
    "                    tf.float32))\n",
    "\n",
    "            # Define optimizer and train step\n",
    "            with tf.name_scope('train'):\n",
    "                gvs = optimizer.compute_gradients(error)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n",
    "                train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "            # Define summaries        \n",
    "            summary_merged = nnu.define_summaries(scalar_sd={'error': error, 'accuracy': accuracy})        \n",
    "\n",
    "    #show_graph(TGraph)        \n",
    "\n",
    "\n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [sz]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, train_step, train_data, valid_data, inputs, targets, path, summary_merged, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[sz] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 120, 60, 1]\n",
      "[50, 30, 3, 50]\n",
      "(4500, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 30, 3, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 4500), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 899.86\n",
      "Test Error: 1.5024868249893188\n",
      "Test Accuracy: 0.46640002727508545\n",
      "[None, 120, 60, 1]\n",
      "[50, 30, 4, 50]\n",
      "(6000, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 30, 4, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 6000), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 1112.0\n",
      "Test Error: 1.4614328145980835\n",
      "Test Accuracy: 0.4791000485420227\n",
      "[None, 120, 60, 1]\n",
      "[50, 30, 5, 50]\n",
      "(7500, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 30, 5, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 7500), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 1342.14\n",
      "Test Error: 1.5275181531906128\n",
      "Test Accuracy: 0.44700005650520325\n",
      "[None, 120, 60, 1]\n",
      "[50, 30, 6, 50]\n",
      "(9000, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 30, 6, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 9000), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 1567.48\n",
      "Test Error: 1.44623601436615\n",
      "Test Accuracy: 0.4878000020980835\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2d_multikernel_2'\n",
    "epochs=20\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "fcParams = [(1500,1000),(1000,train_data.num_classes)]\n",
    "cnPs1 = [(1,5),(5,10)]\n",
    "cnPs2 = [(1,5),(5,10)]\n",
    "cnPs3 = [(1,5),(5,10)]\n",
    "cnPs4 = [(1,5),(5,10)]\n",
    "cnPsO = [(40,50)]\n",
    "\n",
    "kPs = [(3,3),(3,3)]\n",
    "kPsO = [(3,3)]\n",
    "mPPs = [(1,1),(2,1)]\n",
    "mPPsO = [(2,2)]\n",
    "results={}\n",
    "\n",
    "\n",
    "for sz in [6,8,10,12]:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    btcN = True\n",
    "    TGraph=tf.Graph()    \n",
    "\n",
    "    with TGraph.as_default():\n",
    "        layer_outs = []\n",
    "\n",
    "        '''\n",
    "        Set placeholder of targets and inputs.\n",
    "        The inputs placeholder dimensions will be determined by the data.\n",
    "        '''\n",
    "        with tf.name_scope('data'):\n",
    "            targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "            inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2],\n",
    "                                                     train_data.inputs.shape[3]], name='inputs')\n",
    "\n",
    "            input_shape= inputs.get_shape().as_list()\n",
    "            print(input_shape)\n",
    "            inputs1 = tf.slice(inputs, [0,0,0,0], [50, input_shape[1], sz, input_shape[3]]) #timbre\n",
    "            inputs2 = tf.slice(inputs, [0,0,15,0], [50, input_shape[1], sz, input_shape[3]]) #deltas\n",
    "            inputs3 = tf.slice(inputs, [0,0,30,0], [50, input_shape[1], sz, input_shape[3]]) #deltadeltas\n",
    "            inputs4 = tf.slice(inputs, [0,0,45,0], [50, input_shape[1], sz, input_shape[3]]) #chroma\n",
    "\n",
    "            '''\n",
    "            Create Convolutional Layers\n",
    "            '''\n",
    "            conv_outs1 = nnu.multi_convLayers(inputs1, convParams=cnPs1, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')\n",
    "\n",
    "            conv_outs2 = nnu.multi_convLayers(inputs2, convParams=cnPs2, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')\n",
    "\n",
    "            conv_outs3 = nnu.multi_convLayers(inputs3, convParams=cnPs3, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')   \n",
    "\n",
    "            conv_outs4 = nnu.multi_convLayers(inputs4, convParams=cnPs4, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')      \n",
    "\n",
    "\n",
    "            ensemble_out = tf.concat(3, [conv_outs1[-1], conv_outs2[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs3[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs4[-1]])\n",
    "\n",
    "            conv_outsO = nnu.multi_convLayers(ensemble_out, convParams=cnPsO, cnStrides=1, kernelParams=kPsO, \n",
    "                                            maxPoolParams=mPPsO, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')  \n",
    "\n",
    "            shape= conv_outsO[-1].get_shape().as_list()\n",
    "            print(shape)\n",
    "            fcParams[0] = (shape[1]*shape[2]*shape[3], fcParams[0][1])\n",
    "            print(fcParams[0])\n",
    "            print(conv_outsO[-1])\n",
    "            input_data = tf.reshape(conv_outsO[-1], [-1, fcParams[0][0]])\n",
    "            print(input_data)\n",
    "\n",
    "            fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n",
    "            print(fc_outs[-1])\n",
    "            layer_outs.extend(fc_outs)\n",
    "            input_data=layer_outs[-1]\n",
    "\n",
    "            with tf.name_scope('output-layer'):\n",
    "                output = nnu.fully_connected_layer(input_data, fcParams[-1], nonlinearity=tf.identity)\n",
    "                layer_outs.append(output)\n",
    "\n",
    "\n",
    "            # Define the operations for the error\n",
    "            with tf.name_scope('error'):   \n",
    "                error = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(layer_outs[-1], targets))\n",
    "\n",
    "            # Define the function to calculate the accuracy of the system\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(\n",
    "                    tf.equal(tf.argmax(layer_outs[-1], 1), tf.argmax(targets, 1)), \n",
    "                    tf.float32))\n",
    "\n",
    "            # Define optimizer and train step\n",
    "            with tf.name_scope('train'):\n",
    "                gvs = optimizer.compute_gradients(error)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n",
    "                train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "            # Define summaries        \n",
    "            summary_merged = nnu.define_summaries(scalar_sd={'error': error, 'accuracy': accuracy})        \n",
    "\n",
    "    #show_graph(TGraph)        \n",
    "\n",
    "\n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [sz]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, train_step, train_data, valid_data, inputs, targets, path, summary_merged, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[sz] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 120, 60, 1]\n",
      "[50, 1, 1, 50]\n",
      "(50, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 1, 1, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 50), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 339.76\n",
      "Test Error: 1.6028882265090942\n",
      "Test Accuracy: 0.43090003728866577\n",
      "[None, 120, 60, 1]\n",
      "[50, 1, 1, 50]\n",
      "(50, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 1, 1, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 50), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 316.26\n",
      "Test Error: 1.6017009019851685\n",
      "Test Accuracy: 0.43459999561309814\n",
      "[None, 120, 60, 1]\n",
      "[50, 1, 1, 50]\n",
      "(50, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 1, 1, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 50), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 318.36\n",
      "Test Error: 1.6103527545928955\n",
      "Test Accuracy: 0.4351000189781189\n",
      "[None, 120, 60, 1]\n",
      "[50, 1, 1, 50]\n",
      "(50, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 1, 1, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 50), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 319.5\n",
      "Test Error: 1.5801717042922974\n",
      "Test Accuracy: 0.44179999828338623\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv2dff_multikernel_2'\n",
    "epochs=20\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas\n",
    "valid_data = valid_data_fullNoChromaDeltas\n",
    "\n",
    "fcParams = [(1500,1000),(1000,train_data.num_classes)]\n",
    "cnPs1 = [(1,5),(5,10)]\n",
    "cnPs2 = [(1,5),(5,10)]\n",
    "cnPs3 = [(1,5),(5,10)]\n",
    "cnPs4 = [(1,5),(5,10)]\n",
    "cnPsO = [(40,50)]\n",
    "\n",
    "\n",
    "kPsO = [(3,3)]\n",
    "mPPs = [(1,1),(2,1)]\n",
    "mPPsO = [(59,1)]\n",
    "results={}\n",
    "\n",
    "\n",
    "for sz in [6,8,10,12]:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    btcN = True\n",
    "    TGraph=tf.Graph()    \n",
    "    \n",
    "    kPs = [(3,sz),(3,1)]\n",
    "    \n",
    "    with TGraph.as_default():\n",
    "        layer_outs = []\n",
    "\n",
    "        '''\n",
    "        Set placeholder of targets and inputs.\n",
    "        The inputs placeholder dimensions will be determined by the data.\n",
    "        '''\n",
    "        with tf.name_scope('data'):\n",
    "            targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "            inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2],\n",
    "                                                     train_data.inputs.shape[3]], name='inputs')\n",
    "\n",
    "            input_shape= inputs.get_shape().as_list()\n",
    "            print(input_shape)\n",
    "            inputs1 = tf.slice(inputs, [0,0,0,0], [50, input_shape[1], sz, input_shape[3]]) #timbre\n",
    "            inputs2 = tf.slice(inputs, [0,0,15,0], [50, input_shape[1], sz, input_shape[3]]) #deltas\n",
    "            inputs3 = tf.slice(inputs, [0,0,30,0], [50, input_shape[1], sz, input_shape[3]]) #deltadeltas\n",
    "            inputs4 = tf.slice(inputs, [0,0,45,0], [50, input_shape[1], sz, input_shape[3]]) #chroma\n",
    "\n",
    "            '''\n",
    "            Create Convolutional Layers\n",
    "            '''\n",
    "            conv_outs1 = nnu.multi_convLayers(inputs1, convParams=cnPs1, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding=['VALID', 'SAME'])\n",
    "\n",
    "            conv_outs2 = nnu.multi_convLayers(inputs2, convParams=cnPs2, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding=['VALID', 'SAME'])\n",
    "\n",
    "            conv_outs3 = nnu.multi_convLayers(inputs3, convParams=cnPs3, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding=['VALID', 'SAME'])   \n",
    "\n",
    "            conv_outs4 = nnu.multi_convLayers(inputs4, convParams=cnPs4, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding=['VALID', 'SAME'])      \n",
    "\n",
    "\n",
    "            ensemble_out = tf.concat(3, [conv_outs1[-1], conv_outs2[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs3[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs4[-1]])\n",
    "\n",
    "            conv_outsO = nnu.multi_convLayers(ensemble_out, convParams=cnPsO, cnStrides=1, kernelParams=kPsO, \n",
    "                                            maxPoolParams=mPPsO, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')  \n",
    "\n",
    "            shape= conv_outsO[-1].get_shape().as_list()\n",
    "            print(shape)\n",
    "            fcParams[0] = (shape[1]*shape[2]*shape[3], fcParams[0][1])\n",
    "            print(fcParams[0])\n",
    "            print(conv_outsO[-1])\n",
    "            input_data = tf.reshape(conv_outsO[-1], [-1, fcParams[0][0]])\n",
    "            print(input_data)\n",
    "\n",
    "            fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n",
    "            print(fc_outs[-1])\n",
    "            layer_outs.extend(fc_outs)\n",
    "            input_data=layer_outs[-1]\n",
    "\n",
    "            with tf.name_scope('output-layer'):\n",
    "                output = nnu.fully_connected_layer(input_data, fcParams[-1], nonlinearity=tf.identity)\n",
    "                layer_outs.append(output)\n",
    "\n",
    "\n",
    "            # Define the operations for the error\n",
    "            with tf.name_scope('error'):   \n",
    "                error = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(layer_outs[-1], targets))\n",
    "\n",
    "            # Define the function to calculate the accuracy of the system\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(\n",
    "                    tf.equal(tf.argmax(layer_outs[-1], 1), tf.argmax(targets, 1)), \n",
    "                    tf.float32))\n",
    "\n",
    "            # Define optimizer and train step\n",
    "            with tf.name_scope('train'):\n",
    "                gvs = optimizer.compute_gradients(error)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n",
    "                train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "            # Define summaries        \n",
    "            summary_merged = nnu.define_summaries(scalar_sd={'error': error, 'accuracy': accuracy})        \n",
    "\n",
    "    #show_graph(TGraph)        \n",
    "\n",
    "\n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [sz]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, train_step, train_data, valid_data, inputs, targets, path, summary_merged, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[sz] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 120, 60, 1]\n",
      "[50, 15, 6, 50]\n",
      "(4500, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 6, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 4500), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 650.97\n",
      "Test Error: 2.586364507675171\n",
      "Test Accuracy: 0.2239999920129776\n",
      "[None, 120, 60, 1]\n",
      "[50, 15, 8, 50]\n",
      "(6000, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 8, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 6000), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 770.37\n",
      "Test Error: 2.5254673957824707\n",
      "Test Accuracy: 0.2329999953508377\n",
      "[None, 120, 60, 1]\n",
      "[50, 15, 10, 50]\n",
      "(7500, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 10, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 7500), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 918.62\n",
      "Test Error: 2.5207605361938477\n",
      "Test Accuracy: 0.2370000034570694\n",
      "[None, 120, 60, 1]\n",
      "[50, 15, 12, 50]\n",
      "(9000, 1000)\n",
      "Tensor(\"data/mp-layer-0_4/MaxPool:0\", shape=(50, 15, 12, 50), dtype=float32)\n",
      "Tensor(\"data/Reshape:0\", shape=(50, 9000), dtype=float32)\n",
      "Tensor(\"data/fc-layer-0/Elu:0\", shape=(50, 1000), dtype=float32)\n",
      "Runtime: 1057.29\n",
      "Test Error: 2.4867067337036133\n",
      "Test Accuracy: 0.24500000476837158\n"
     ]
    }
   ],
   "source": [
    "'''Conv2D'''\n",
    "testname='conv1d_multikernel_2_25'\n",
    "epochs=20\n",
    "\n",
    "train_data = train_data_fullNoChromaDeltas25\n",
    "valid_data = valid_data_fullNoChromaDeltas25\n",
    "\n",
    "fcParams = [(1500,1000),(1000,train_data.num_classes)]\n",
    "cnPs1 = [(1,5),(5,10)]\n",
    "cnPs2 = [(1,5),(5,10)]\n",
    "cnPs3 = [(1,5),(5,10)]\n",
    "cnPs4 = [(1,5),(5,10)]\n",
    "cnPsO = [(40,50)]\n",
    "\n",
    "kPs = [(5,1),(5,1)]\n",
    "kPsO = [(5,1)]\n",
    "mPPs = [(2,1),(2,1)]\n",
    "mPPsO = [(2,1)]\n",
    "results={}\n",
    "\n",
    "\n",
    "for sz in [6,8,10,12]:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    btcN = True\n",
    "    TGraph=tf.Graph()    \n",
    "\n",
    "    with TGraph.as_default():\n",
    "        layer_outs = []\n",
    "\n",
    "        '''\n",
    "        Set placeholder of targets and inputs.\n",
    "        The inputs placeholder dimensions will be determined by the data.\n",
    "        '''\n",
    "        with tf.name_scope('data'):\n",
    "            targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')\n",
    "            inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2],\n",
    "                                                     train_data.inputs.shape[3]], name='inputs')\n",
    "\n",
    "            input_shape= inputs.get_shape().as_list()\n",
    "            print(input_shape)\n",
    "            inputs1 = tf.slice(inputs, [0,0,0,0], [50, input_shape[1], sz, input_shape[3]]) #timbre\n",
    "            inputs2 = tf.slice(inputs, [0,0,15,0], [50, input_shape[1], sz, input_shape[3]]) #deltas\n",
    "            inputs3 = tf.slice(inputs, [0,0,30,0], [50, input_shape[1], sz, input_shape[3]]) #deltadeltas\n",
    "            inputs4 = tf.slice(inputs, [0,0,45,0], [50, input_shape[1], sz, input_shape[3]]) #chroma\n",
    "\n",
    "            '''\n",
    "            Create Convolutional Layers\n",
    "            '''\n",
    "            conv_outs1 = nnu.multi_convLayers(inputs1, convParams=cnPs1, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')\n",
    "\n",
    "            conv_outs2 = nnu.multi_convLayers(inputs2, convParams=cnPs2, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')\n",
    "\n",
    "            conv_outs3 = nnu.multi_convLayers(inputs3, convParams=cnPs3, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')   \n",
    "\n",
    "            conv_outs4 = nnu.multi_convLayers(inputs4, convParams=cnPs4, cnStrides=1, kernelParams=kPs, \n",
    "                                            maxPoolParams=mPPs, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')      \n",
    "\n",
    "\n",
    "            ensemble_out = tf.concat(3, [conv_outs1[-1], conv_outs2[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs3[-1]])\n",
    "            ensemble_out = tf.concat(3, [ensemble_out, conv_outs4[-1]])\n",
    "\n",
    "            conv_outsO = nnu.multi_convLayers(ensemble_out, convParams=cnPsO, cnStrides=1, kernelParams=kPsO, \n",
    "                                            maxPoolParams=mPPsO, cnNonLinearity=tf.nn.elu, dropPCn=1, \n",
    "                                            bnCn=btcN, padding='SAME')  \n",
    "\n",
    "            shape= conv_outsO[-1].get_shape().as_list()\n",
    "            print(shape)\n",
    "            fcParams[0] = (shape[1]*shape[2]*shape[3], fcParams[0][1])\n",
    "            print(fcParams[0])\n",
    "            print(conv_outsO[-1])\n",
    "            input_data = tf.reshape(conv_outsO[-1], [-1, fcParams[0][0]])\n",
    "            print(input_data)\n",
    "\n",
    "            fc_outs = nnu.multi_fcLayers(input_data, fcParams=fcParams[:-1], fcNonLinearity=tf.nn.elu, dropPFc=1, bnFc=False)\n",
    "            print(fc_outs[-1])\n",
    "            layer_outs.extend(fc_outs)\n",
    "            input_data=layer_outs[-1]\n",
    "\n",
    "            with tf.name_scope('output-layer'):\n",
    "                output = nnu.fully_connected_layer(input_data, fcParams[-1], nonlinearity=tf.identity)\n",
    "                layer_outs.append(output)\n",
    "\n",
    "\n",
    "            # Define the operations for the error\n",
    "            with tf.name_scope('error'):   \n",
    "                error = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(layer_outs[-1], targets))\n",
    "\n",
    "            # Define the function to calculate the accuracy of the system\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(\n",
    "                    tf.equal(tf.argmax(layer_outs[-1], 1), tf.argmax(targets, 1)), \n",
    "                    tf.float32))\n",
    "\n",
    "            # Define optimizer and train step\n",
    "            with tf.name_scope('train'):\n",
    "                gvs = optimizer.compute_gradients(error)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -2., 2.), var) for grad, var in gvs]\n",
    "                train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "            # Define summaries        \n",
    "            summary_merged = nnu.define_summaries(scalar_sd={'error': error, 'accuracy': accuracy})        \n",
    "\n",
    "    #show_graph(TGraph)        \n",
    "\n",
    "\n",
    "    # Perform test and save results in dictionary\n",
    "    path = auu.create_fname(testname, [sz]) # <<\n",
    "    start_run = time.time()\n",
    "    test_error, test_accuracy = run_session(TGraph, train_step, train_data, valid_data, inputs, targets, path, summary_merged, epochs)\n",
    "    runtime = round(time.time() - start_run, 2)\n",
    "    results[sz] = [runtime, test_error, test_accuracy] #<<\n",
    "\n",
    "    # Print results to console\n",
    "    print(\"Runtime: {0}\".format(runtime))\n",
    "    print(\"Test Error: {0}\\nTest Accuracy: {1}\".format(test_error, test_accuracy))\n",
    "    \n",
    "    \n",
    "# save CSV file and print final    \n",
    "auu.dict2csv(results, '../notebooks/Coursework4/results/', testname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_20:0\", shape=(1, 4, 6, 1), dtype=int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-66e10902786c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m   \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   2866\u001b[0m   \"\"\"\n\u001b[1;32m   2867\u001b[0m   result = _op_def_lib.apply_op(\"Slice\", input=input, begin=begin, size=size,\n\u001b[0;32m-> 2868\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m   2869\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    491\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m               \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m               raise TypeError(\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    491\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    174\u001b[0m                                          as_ref=False):\n\u001b[1;32m    175\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m--> 165\u001b[0;31m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnumpy_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mproto_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FlattenToStrings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Mark\\Anaconda3\\envs\\mlp35\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 65\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got None"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[[[1],[2],[3],[4],[5],[6]], [[1],[2],[3],[4],[5],[6]], [[1],[2],[3],[4],[5],[6]], [[1],[2],[3],[4],[5],[6]]]])\n",
    "print(a)\n",
    "\n",
    "b = tf.slice(a, [0,0,2,0], [, a.get_shape().as_list()[1], 2, a.get_shape().as_list()[3]])\n",
    "print(b)\n",
    "\n",
    "\n",
    "tsess = tf.Session()\n",
    "result1, result2 = tsess.run([a,b])\n",
    "print(result1)\n",
    "print(\" \")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
