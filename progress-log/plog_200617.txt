Notes:

1. Test one using the same structure as Dieleman et al. 

Test name: output_ds256_04

Structure:	Strided Convolutional layer - KS256, SR64, FILT1
			1st Convolutional layer - KS8, SR8, FILT32
			Max Pooling layer - MS4, SR8
			2nd Convolutional layer - KS8, SR8, FILT32
			Fully connected layer - HU100
			Fully connected layer - HU50
			
Analysis of results:
In the results it shows that the number of false positives and true positives go very low very quickly and
reach zero in the first few thousand steps of the test.
Probably, the reason for this is that the model learns to give mostly low probabilities since the data is 
spase. However, this also leads to give less and less high probabilities. 

Analysis tests: Try to do tests with merged target tags. The oness explain by Choi et al on the Github
				notebook. The merging of the tags would reduce the sparisity which in turn might be enough
				for the model to adapt better and get better precision. 

This low precision was not seen be Dieleman et al. as they used a windowed model where the graph accepts windowd
samples and the tags predicted are averaged for the error. 

Possible solutions:	1. Use a windowed approach. 
					2. Increase complexity of the model.
					
					
Tests to be conducted:	1. Test with reduced tags chosen intelligently	
						2. Test with merged tags
						3. Test with FBANKS no windowing
						4. Test with RAW with windowing 
						5. Test with FBANKS with windowing